#LSA
from sklearn.decomposition import TruncatedSVD
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import Normalizer
from sklearn.decomposition import LatentDirichletAllocation as LDA
from sklearn.decomposition import NMF

import pandas as pd
import numpy as np


# Linking words to topics
def word_topic(vec_data, unsuper_method, terms):

	#terms = vectorizor.get_feature_names()
    
    # Loading scores for each word on each topic/component.
	words_by_topic=vec_data.T * unsuper_method

    # Linking the loadings to the words in an easy-to-read way.
	components=pd.DataFrame(words_by_topic,index=terms)
    
	return components

def top_words(components, n_top_words):
	n_topics = range(components.shape[1])
	index= np.repeat(n_topics, n_top_words, axis=0)
	topwords=pd.Series(index=index)
	for column in range(components.shape[1]):
	    # Sort the column so that highest loadings are at the top.
	    sortedwords=components.iloc[:,column].sort_values(ascending=False)
	    # Choose the N highest loadings.
	    chosen=sortedwords[:n_top_words]
	    # Combine loading and index into a string.
	    chosenlist=chosen.index +" "+round(chosen,2).map(str) 
	    topwords.loc[column]=[x for x in chosenlist]
	return(topwords)


def lda_pipeline(vec_data, ntopics, n_top_words, vectorizor):
	'''
	takes:
		vec_data --> vectorized data ex: tfidf, bow
		ntopics --> integer, number of topics ex: 5
		n_top_words --> integer, number of words to look for in each topic ex: 10
		vectorizer --> instance, an instance of the vectorizer ex: tfidf, bow

	'''

	#instantiate LDA 
	lda = LDA(n_components=ntopics, 
          doc_topic_prior=None, # Prior = 1/n_documents
          topic_word_prior=1/ntopics,
          learning_decay=0.7, # Convergence rate.
          learning_offset=10.0, # Causes earlier iterations to have less influence on the learning
          max_iter=10, # when to stop even if the model is not converging (to prevent running forever)
          evaluate_every=-1, # Do not evaluate perplexity, as it slows training time.
          mean_change_tol=0.001, # Stop updating the document topic distribution in the E-step when mean change is < tol
          max_doc_update_iter=100, # When to stop updating the document topic distribution in the E-step even if tol is not reached
          n_jobs=-1, # Use all available CPUs to speed up processing time.
          verbose=0, # amount of output to give while iterating
          random_state=0
        )

	#fit transform the data
	data_lda = lda.fit_transform(vec_data)

	terms=vectorizor.get_feature_names()

	#link the words to topics
	components_lda = word_topic(vec_data, data_lda, terms)

	df = pd.DataFrame()

	#extract top N words and their loadings for each topic
	df['LDA']=top_words(components_lda, n_top_words)

	return df


#returns components 
def lsa_pipeline(vec_data, ntopics, n_top_words, vectorizor):
	'''
	takes in vectorized data, topic numbers, and a vectorizor instance
	returns dataframe with terms and scores for each topic

	'''
	svd = TruncatedSVD(ntopics)
	lsa = make_pipeline(svd, Normalizer(copy=False))
	data_lsa = lsa.fit_transform(vec_data)

	#getting the word list
	terms = vectorizor.get_feature_names()

	#loading scores for each word on each topic/component
	components_lsa = word_topic(vec_data, data_lsa, terms)

	#linking the loadings to the words in an easy to read way

	df=pd.DataFrame()

	df['LSA']=top_words(components_lsa, n_top_words)

	return df 


def nmf_pipeline(vec_data, ntopics, n_top_words, vectorizor):
	'''
	takes:
		vec_data --> vectorized data ex: tfidf, bow
		ntopics --> integer, number of topics ex: 5
		n_top_words --> integer, number of words to look for in each topic ex: 10
		vectorizer --> instance, an instance of the vectorizer ex: tfidf, bow

	'''

	#instantiate LDA 
	nmf = NMF(alpha=0.0, 
          init='nndsvdar', # how starting value are calculated
          l1_ratio=0.0, # Sets whether regularization is L2 (0), L1 (1), or a combination (values between 0 and 1)
          max_iter=200, # when to stop even if the model is not converging (to prevent running forever)
          n_components=ntopics, 
          random_state=0, 
          solver='cd', # Use Coordinate Descent to solve
          tol=0.0001, # model will stop if tfidf-WH <= tol
          verbose=0 # amount of output to give while iterating
         )

	#fit transform the data
	data_nmf = nmf.fit_transform(vec_data)

	terms=vectorizor.get_feature_names()

	#link the words to topics
	components_nmf = word_topic(vec_data, data_nmf, terms)

	df = pd.DataFrame()

	#extract top N words and their loadings for each topic
	df['NMF']=top_words(components_nmf, n_top_words)

	return df